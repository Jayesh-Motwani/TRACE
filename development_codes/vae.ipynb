{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-03T10:30:30.305010Z",
     "start_time": "2026-02-03T10:30:29.082609Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import langchain\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import kagglehub"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T04:43:43.808742Z",
     "start_time": "2026-02-03T04:43:41.724990Z"
    }
   },
   "cell_type": "code",
   "source": "torch.cuda.is_available()",
   "id": "b64302a14ba45dbf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T04:52:58.533704Z",
     "start_time": "2026-02-03T04:50:12.950779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"solarmainframe/ids-intrusion-csv\")"
   ],
   "id": "2b027094827ab06d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming download from 125829120 bytes (1591072049 bytes left)...\n",
      "Resuming download to C:\\Users\\mjaye\\.cache\\kagglehub\\datasets\\solarmainframe\\ids-intrusion-csv\\1.archive (125829120/1716901169) bytes left.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.60G/1.60G [02:18<00:00, 11.5MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\mjaye\\.cache\\kagglehub\\datasets\\solarmainframe\\ids-intrusion-csv\\versions\\1\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T05:04:08.236558Z",
     "start_time": "2026-02-03T05:04:08.232557Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "8eaeff65dd91b042",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjaye\\PycharmProjects\\TRACE\n",
      "exists: False\n",
      "isfile: False\n",
      "readable: False\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T08:42:33.382955Z",
     "start_time": "2026-02-03T08:42:33.379952Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "95ca4fd3f88fac3c",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T05:08:34.199926Z",
     "start_time": "2026-02-03T05:08:34.190466Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "1fe4257d931b5c44",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of          Dst Port  Protocol            Timestamp  Flow Duration  Tot Fwd Pkts  \\\n",
       "0             443         6  02/03/2018 08:47:38         141385             9   \n",
       "1           49684         6  02/03/2018 08:47:38            281             2   \n",
       "2             443         6  02/03/2018 08:47:40         279824            11   \n",
       "3             443         6  02/03/2018 08:47:40            132             2   \n",
       "4             443         6  02/03/2018 08:47:41         274016             9   \n",
       "...           ...       ...                  ...            ...           ...   \n",
       "1048570      3389         6  02/03/2018 02:08:18        3982183            14   \n",
       "1048571      3389         6  02/03/2018 02:08:22        3802316            14   \n",
       "1048572      3389         6  02/03/2018 02:08:25        4004239            14   \n",
       "1048573      3389         6  02/03/2018 02:08:29        3998435            14   \n",
       "1048574      3389         6  02/03/2018 02:08:33        3972651            14   \n",
       "\n",
       "         Tot Bwd Pkts  TotLen Fwd Pkts  TotLen Bwd Pkts  Fwd Pkt Len Max  \\\n",
       "0                   7              553           3773.0              202   \n",
       "1                   1               38              0.0               38   \n",
       "2                  15             1086          10527.0              385   \n",
       "3                   0                0              0.0                0   \n",
       "4                  13             1285           6141.0              517   \n",
       "...               ...              ...              ...              ...   \n",
       "1048570             8             1442           1731.0              725   \n",
       "1048571             8             1440           1731.0              725   \n",
       "1048572             8             1459           1731.0              741   \n",
       "1048573             8             1459           1731.0              741   \n",
       "1048574             8             1439           1731.0              725   \n",
       "\n",
       "         Fwd Pkt Len Min  ...  Fwd Seg Size Min  Active Mean  Active Std  \\\n",
       "0                      0  ...                20          0.0         0.0   \n",
       "1                      0  ...                20          0.0         0.0   \n",
       "2                      0  ...                20          0.0         0.0   \n",
       "3                      0  ...                20          0.0         0.0   \n",
       "4                      0  ...                20          0.0         0.0   \n",
       "...                  ...  ...               ...          ...         ...   \n",
       "1048570                0  ...                20          0.0         0.0   \n",
       "1048571                0  ...                20          0.0         0.0   \n",
       "1048572                0  ...                20          0.0         0.0   \n",
       "1048573                0  ...                20          0.0         0.0   \n",
       "1048574                0  ...                20          0.0         0.0   \n",
       "\n",
       "         Active Max  Active Min  Idle Mean  Idle Std  Idle Max  Idle Min  \\\n",
       "0               0.0         0.0        0.0       0.0       0.0       0.0   \n",
       "1               0.0         0.0        0.0       0.0       0.0       0.0   \n",
       "2               0.0         0.0        0.0       0.0       0.0       0.0   \n",
       "3               0.0         0.0        0.0       0.0       0.0       0.0   \n",
       "4               0.0         0.0        0.0       0.0       0.0       0.0   \n",
       "...             ...         ...        ...       ...       ...       ...   \n",
       "1048570         0.0         0.0        0.0       0.0       0.0       0.0   \n",
       "1048571         0.0         0.0        0.0       0.0       0.0       0.0   \n",
       "1048572         0.0         0.0        0.0       0.0       0.0       0.0   \n",
       "1048573         0.0         0.0        0.0       0.0       0.0       0.0   \n",
       "1048574         0.0         0.0        0.0       0.0       0.0       0.0   \n",
       "\n",
       "          Label  \n",
       "0        Benign  \n",
       "1        Benign  \n",
       "2        Benign  \n",
       "3        Benign  \n",
       "4        Benign  \n",
       "...         ...  \n",
       "1048570  Benign  \n",
       "1048571  Benign  \n",
       "1048572  Benign  \n",
       "1048573  Benign  \n",
       "1048574  Benign  \n",
       "\n",
       "[1048575 rows x 80 columns]>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T09:33:53.786815Z",
     "start_time": "2026-02-03T09:33:53.549743Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, accuracy_score,\n",
    "                            precision_recall_fscore_support)\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "from collections import Counter\n",
    "import pickle"
   ],
   "id": "d6a1329c11cbeabd",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T09:33:55.080136Z",
     "start_time": "2026-02-03T09:33:55.076284Z"
    }
   },
   "cell_type": "code",
   "source": "path = \"C:\\\\Users\\\\mjaye\\\\PycharmProjects\\\\TRACE\\\\solarmainframe\\\\ids-intrusion-csv\\\\versions\\\\1\"",
   "id": "83ba316b469f8620",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T09:33:55.580894Z",
     "start_time": "2026-02-03T09:33:55.577495Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _create_memmap_windows_safe(filename, shape, dtype):\n",
    "        if os.path.exists(filename):\n",
    "            os.remove(filename)\n",
    "\n",
    "        dtype = np.dtype(dtype)\n",
    "        bytes_needed = int(np.prod(shape)) * dtype.itemsize\n",
    "\n",
    "        # Pre-allocate file size\n",
    "        with open(filename, \"wb\") as f:\n",
    "            f.seek(bytes_needed - 1)\n",
    "            f.write(b\"\\0\")\n",
    "\n",
    "        # Map existing file\n",
    "        return np.memmap(filename, dtype=dtype, mode=\"r+\", shape=shape)"
   ],
   "id": "6942e5e2f79db278",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T09:33:55.968276Z",
     "start_time": "2026-02-03T09:33:55.954100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "class FullDatasetLoader:\n",
    "    \"\"\"Loads ALL 10 CSV files with ALL features using memory mapping\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path, chunk_size=100000):\n",
    "        self.data_path = data_path\n",
    "        self.chunk_size = chunk_size\n",
    "        self.scaler = None\n",
    "        self.label_encoder = None\n",
    "        self.feature_names = None\n",
    "        self.num_features = None\n",
    "        \n",
    "    def load_full_dataset(self):\n",
    "        \"\"\"Load ALL 10 CSV files with ALL features\"\"\"\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"LOADING FULL DATASET (ALL 10 CSV FILES)\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        csv_files = sorted([f for f in os.listdir(self.data_path) if f.endswith('.csv')])\n",
    "        print(f\"Found {len(csv_files)} CSV files\")\n",
    "        print(f\"Files: {csv_files}\")\n",
    "        \n",
    "        # First pass: collect metadata and determine feature names\n",
    "        print(\"\\n Pass 1: Collecting metadata from all files...\")\n",
    "        all_columns = None\n",
    "        total_rows = 0\n",
    "        \n",
    "        for i, file in enumerate(csv_files, 1):\n",
    "            filepath = os.path.join(self.data_path, file)\n",
    "            print(f\"Scanning {i}/{len(csv_files)}: {file}\")\n",
    "            \n",
    "            # Read just first chunk to get columns\n",
    "            df_sample = pd.read_csv(filepath, nrows=1000)\n",
    "            df_sample = df_sample[df_sample['Label'] != 'Label']\n",
    "            \n",
    "            if all_columns is None:\n",
    "                all_columns = df_sample.columns.tolist()\n",
    "            \n",
    "            # Count total rows\n",
    "            row_count = sum(1 for _ in open(filepath)) - 1  # -1 for header\n",
    "            total_rows += row_count\n",
    "            print(f\"  Rows: {row_count:,}\")\n",
    "        \n",
    "        print(f\"\\n Total rows across all files: {total_rows:,}\")\n",
    "        print(f\" Total columns: {len(all_columns)}\")\n",
    "        \n",
    "        # Remove non-feature columns\n",
    "        cols_to_remove = ['Timestamp', 'Flow ID', 'Source IP', 'Destination IP', \n",
    "                         'Source Port', 'Destination Port', 'Protocol']\n",
    "        feature_cols = [c for c in all_columns if c not in cols_to_remove and c != 'Label']\n",
    "        \n",
    "        self.feature_names = feature_cols\n",
    "        self.num_features = len(feature_cols)\n",
    "        \n",
    "        print(f\" Feature columns (ALL): {self.num_features}\")\n",
    "        print(f\" Using ALL features - NO feature selection\")\n",
    "        \n",
    "        # Second pass: Load all data in chunks and save to memory-mapped file\n",
    "        print(\"\\n Pass 2: Loading all data and creating memory-mapped arrays...\")\n",
    "        \n",
    "        # Create temporary memory-mapped files\n",
    "        X_memmap = _create_memmap_windows_safe(\"X_temp.dat\", (total_rows, self.num_features), np.float32)\n",
    "        y_memmap = _create_memmap_windows_safe(\"y_temp.dat\", (total_rows,), np.uint8)\n",
    "        current_idx = 0\n",
    "        \n",
    "        for i, file in enumerate(csv_files, 1):\n",
    "            filepath = os.path.join(self.data_path, file)\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Processing {i}/{len(csv_files)}: {file}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            chunk_iter = pd.read_csv(filepath, chunksize=self.chunk_size)\n",
    "            file_rows = 0\n",
    "            \n",
    "            for chunk_num, chunk in enumerate(chunk_iter):\n",
    "                # Clean chunk\n",
    "                chunk = chunk[chunk['Label'] != 'Label']\n",
    "                \n",
    "                if len(chunk) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Drop non-feature columns\n",
    "                chunk = chunk.drop(columns=[c for c in cols_to_remove if c in chunk.columns], \n",
    "                                  errors='ignore')\n",
    "                \n",
    "                # Separate features and labels\n",
    "                labels = chunk['Label'].astype(str).str.strip()\n",
    "                y_chunk = (labels != 'BENIGN').astype(np.uint8)  # 0 benign, 1 attack\n",
    "                X_chunk = chunk[feature_cols]\n",
    "                \n",
    "                # Convert to numeric\n",
    "                X_chunk = X_chunk.apply(pd.to_numeric, errors='coerce')\n",
    "                X_chunk = X_chunk.replace([np.inf, -np.inf], np.nan)\n",
    "                X_chunk = X_chunk.fillna(0)\n",
    "                \n",
    "                # Store in memmap\n",
    "                n_samples = len(X_chunk)\n",
    "                X_memmap[current_idx:current_idx + n_samples] = X_chunk.values.astype(np.float32)\n",
    "                y_memmap[current_idx:current_idx + n_samples] = y_chunk\n",
    "                \n",
    "                current_idx += n_samples\n",
    "                file_rows += n_samples\n",
    "                \n",
    "                if chunk_num % 10 == 0:\n",
    "                    print(f\"  Chunk {chunk_num}: Processed {current_idx:,}/{total_rows:,} rows\")\n",
    "                \n",
    "                # Clean up\n",
    "                del chunk, X_chunk, y_chunk\n",
    "                gc.collect()\n",
    "            \n",
    "            print(f\" File {i} complete: {file_rows:,} rows processed\")\n",
    "        \n",
    "        print(f\"\\n Total rows loaded: {current_idx:,}\")\n",
    "        \n",
    "        # Trim to actual size\n",
    "        X_final = X_memmap[:current_idx]\n",
    "        y_final = y_memmap[:current_idx]\n",
    "        \n",
    "        # Flush to disk\n",
    "        X_final.flush()\n",
    "        y_final.flush()\n",
    "        \n",
    "        return X_final, y_final, current_idx\n",
    "    \n",
    "    def fit_scaler_on_memmap(self, X_memmap, y_memmap, total_samples):\n",
    "        \"\"\"Fit scaler incrementally on memory-mapped data\"\"\"\n",
    "        self.scaler = StandardScaler()\n",
    "        chunk_size = 100000\n",
    "        for i in range(0, total_samples, chunk_size):\n",
    "            end = min(i + chunk_size, total_samples)\n",
    "            Xc = X_memmap[i:end]\n",
    "            yc = y_memmap[i:end]\n",
    "            benign_mask = (yc == 0)\n",
    "            if benign_mask.any():\n",
    "                self.scaler.partial_fit(Xc[benign_mask])\n",
    "        return self.scaler\n",
    "    \n",
    "    def transform_and_save_windows_safe(self, X_memmap, total_samples, out_file=\"X_scaled.dat\"):\n",
    "        print(\"\\n Scaling ALL data...\")\n",
    "\n",
    "        # Remove old file if it exists (avoids “invalid argument” / locking issues)\n",
    "        if os.path.exists(out_file):\n",
    "            os.remove(out_file)\n",
    "\n",
    "        total_samples = int(total_samples)\n",
    "        num_features = int(self.num_features)\n",
    "        dtype = np.float32\n",
    "        bytes_needed = total_samples * num_features * np.dtype(dtype).itemsize\n",
    "\n",
    "        # Pre-allocate file to exact size\n",
    "        with open(out_file, \"wb\") as f:\n",
    "            f.seek(bytes_needed - 1)\n",
    "            f.write(b\"\\0\")\n",
    "\n",
    "        # Now memmap with r+ (read/write existing)\n",
    "        X_scaled = np.memmap(out_file, dtype=dtype, mode=\"r+\", shape=(total_samples, num_features))\n",
    "\n",
    "        chunk_size = 100000\n",
    "        for i in range(0, total_samples, chunk_size):\n",
    "            end_idx = min(i + chunk_size, total_samples)\n",
    "            chunk = X_memmap[i:end_idx]\n",
    "            X_scaled[i:end_idx] = self.scaler.transform(chunk).astype(dtype)\n",
    "\n",
    "            if i % 500000 == 0:\n",
    "                print(f\"  Scaled {i:,}/{total_samples:,} samples...\")\n",
    "\n",
    "        X_scaled.flush()\n",
    "        print(\" All data scaled and saved to\", out_file)\n",
    "        return X_scaled"
   ],
   "id": "af04e09f3c8d38b8",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T09:36:47.382482Z",
     "start_time": "2026-02-03T09:33:56.561316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_loader = FullDatasetLoader(data_path=path, chunk_size=100000)\n",
    "X_raw, y, n = dataset_loader.load_full_dataset()\n",
    "print(X_raw.shape, y.shape, n)"
   ],
   "id": "e5989527800d7025",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LOADING FULL DATASET (ALL 10 CSV FILES)\n",
      "================================================================================\n",
      "Found 10 CSV files\n",
      "Files: ['02-14-2018.csv', '02-15-2018.csv', '02-16-2018.csv', '02-20-2018.csv', '02-21-2018.csv', '02-22-2018.csv', '02-23-2018.csv', '02-28-2018.csv', '03-01-2018.csv', '03-02-2018.csv']\n",
      "\n",
      " Pass 1: Collecting metadata from all files...\n",
      "Scanning 1/10: 02-14-2018.csv\n",
      "  Rows: 1,048,575\n",
      "Scanning 2/10: 02-15-2018.csv\n",
      "  Rows: 1,048,575\n",
      "Scanning 3/10: 02-16-2018.csv\n",
      "  Rows: 1,048,575\n",
      "Scanning 4/10: 02-20-2018.csv\n",
      "  Rows: 7,948,748\n",
      "Scanning 5/10: 02-21-2018.csv\n",
      "  Rows: 1,048,575\n",
      "Scanning 6/10: 02-22-2018.csv\n",
      "  Rows: 1,048,575\n",
      "Scanning 7/10: 02-23-2018.csv\n",
      "  Rows: 1,048,575\n",
      "Scanning 8/10: 02-28-2018.csv\n",
      "  Rows: 613,104\n",
      "Scanning 9/10: 03-01-2018.csv\n",
      "  Rows: 331,125\n",
      "Scanning 10/10: 03-02-2018.csv\n",
      "  Rows: 1,048,575\n",
      "\n",
      " Total rows across all files: 16,233,002\n",
      " Total columns: 80\n",
      " Feature columns (ALL): 77\n",
      " Using ALL features - NO feature selection\n",
      "\n",
      " Pass 2: Loading all data and creating memory-mapped arrays...\n",
      "\n",
      "============================================================\n",
      "Processing 1/10: 02-14-2018.csv\n",
      "============================================================\n",
      "  Chunk 0: Processed 100,000/16,233,002 rows\n",
      "  Chunk 10: Processed 1,048,575/16,233,002 rows\n",
      " File 1 complete: 1,048,575 rows processed\n",
      "\n",
      "============================================================\n",
      "Processing 2/10: 02-15-2018.csv\n",
      "============================================================\n",
      "  Chunk 0: Processed 1,148,575/16,233,002 rows\n",
      "  Chunk 10: Processed 2,097,150/16,233,002 rows\n",
      " File 2 complete: 1,048,575 rows processed\n",
      "\n",
      "============================================================\n",
      "Processing 3/10: 02-16-2018.csv\n",
      "============================================================\n",
      "  Chunk 0: Processed 2,197,150/16,233,002 rows\n",
      "  Chunk 10: Processed 3,145,724/16,233,002 rows\n",
      " File 3 complete: 1,048,574 rows processed\n",
      "\n",
      "============================================================\n",
      "Processing 4/10: 02-20-2018.csv\n",
      "============================================================\n",
      "  Chunk 0: Processed 3,245,724/16,233,002 rows\n",
      "  Chunk 10: Processed 4,245,724/16,233,002 rows\n",
      "  Chunk 20: Processed 5,245,724/16,233,002 rows\n",
      "  Chunk 30: Processed 6,245,724/16,233,002 rows\n",
      "  Chunk 40: Processed 7,245,724/16,233,002 rows\n",
      "  Chunk 50: Processed 8,245,724/16,233,002 rows\n",
      "  Chunk 60: Processed 9,245,724/16,233,002 rows\n",
      "  Chunk 70: Processed 10,245,724/16,233,002 rows\n",
      " File 4 complete: 7,948,748 rows processed\n",
      "\n",
      "============================================================\n",
      "Processing 5/10: 02-21-2018.csv\n",
      "============================================================\n",
      "  Chunk 0: Processed 11,194,472/16,233,002 rows\n",
      "  Chunk 10: Processed 12,143,047/16,233,002 rows\n",
      " File 5 complete: 1,048,575 rows processed\n",
      "\n",
      "============================================================\n",
      "Processing 6/10: 02-22-2018.csv\n",
      "============================================================\n",
      "  Chunk 0: Processed 12,243,047/16,233,002 rows\n",
      "  Chunk 10: Processed 13,191,622/16,233,002 rows\n",
      " File 6 complete: 1,048,575 rows processed\n",
      "\n",
      "============================================================\n",
      "Processing 7/10: 02-23-2018.csv\n",
      "============================================================\n",
      "  Chunk 0: Processed 13,291,622/16,233,002 rows\n",
      "  Chunk 10: Processed 14,240,197/16,233,002 rows\n",
      " File 7 complete: 1,048,575 rows processed\n",
      "\n",
      "============================================================\n",
      "Processing 8/10: 02-28-2018.csv\n",
      "============================================================\n",
      "  Chunk 0: Processed 14,340,193/16,233,002 rows\n",
      " File 8 complete: 613,071 rows processed\n",
      "\n",
      "============================================================\n",
      "Processing 9/10: 03-01-2018.csv\n",
      "============================================================\n",
      "  Chunk 0: Processed 14,953,256/16,233,002 rows\n",
      " File 9 complete: 331,100 rows processed\n",
      "\n",
      "============================================================\n",
      "Processing 10/10: 03-02-2018.csv\n",
      "============================================================\n",
      "  Chunk 0: Processed 15,284,368/16,233,002 rows\n",
      "  Chunk 10: Processed 16,232,943/16,233,002 rows\n",
      " File 10 complete: 1,048,575 rows processed\n",
      "\n",
      " Total rows loaded: 16,232,943\n",
      "(16232943, 77) (16232943,) 16232943\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T09:50:29.325353Z",
     "start_time": "2026-02-03T09:50:29.288902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_raw.flush()\n",
    "y.flush()\n",
    "print(\"Saved/Flushed:\", X_raw.filename, y.filename)"
   ],
   "id": "f2fb3effa6b468ed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved/Flushed: C:\\Users\\mjaye\\PycharmProjects\\TRACE\\X_temp.dat C:\\Users\\mjaye\\PycharmProjects\\TRACE\\y_temp.dat\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T09:50:30.980669Z",
     "start_time": "2026-02-03T09:50:30.174293Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, gc\n",
    "\n",
    "x_path = X_raw.filename\n",
    "y_path = y.filename\n",
    "\n",
    "# close memmaps cleanly\n",
    "del X_raw\n",
    "del y\n",
    "gc.collect()\n",
    "\n",
    "# rename (move) to stable names\n",
    "os.rename(x_path, \"X_raw_final.dat\")\n",
    "os.rename(y_path, \"y_final.dat\")\n",
    "\n",
    "print(\"Renamed to X_raw_final.dat and y_final.dat\")"
   ],
   "id": "47d075f657c8456a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed to X_raw_final.dat and y_final.dat\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T09:51:56.107546Z",
     "start_time": "2026-02-03T09:51:56.101152Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "n = 16233002          # your saved n\n",
    "num_features = 77     # your saved feature count\n",
    "\n",
    "X_raw = np.memmap(\"X_raw_final.dat\", dtype=\"float32\", mode=\"r\", shape=(n, num_features))\n",
    "y     = np.memmap(\"y_final.dat\",     dtype=\"uint8\",   mode=\"r\", shape=(n,))\n",
    "print(X_raw.shape, y.shape)"
   ],
   "id": "8289d9a010be9403",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16233002, 77) (16233002,)\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T09:53:10.842398Z",
     "start_time": "2026-02-03T09:53:10.802292Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"X:\", X_raw.shape, X_raw.dtype)\n",
    "print(\"y:\", y.shape, y.dtype)\n",
    "print(\"benign:\", int((y==0).sum()), \"attack:\", int((y==1).sum()))\n",
    "print(\"X min/max (sample):\", float(X_raw[:10000].min()), float(X_raw[:10000].max()))"
   ],
   "id": "6bdd5440ba9351c9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (16233002, 77) float32\n",
      "y: (16233002,) uint8\n",
      "benign: 59 attack: 16232943\n",
      "X min/max (sample): -1.0 112642176.0\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T09:54:47.328014Z",
     "start_time": "2026-02-03T09:54:44.698507Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd, os\n",
    "\n",
    "one_file = os.path.join(path, \"02-14-2018.csv\")\n",
    "s = pd.read_csv(one_file, usecols=[\"Label\"]).iloc[:200000, 0].astype(str)\n",
    "\n",
    "print(\"raw uniques (top):\")\n",
    "print(s.value_counts().head(20))\n",
    "\n",
    "print(\"\\nnormalized uniques (top):\")\n",
    "sn = s.str.strip().str.upper()\n",
    "print(sn.value_counts().head(20))"
   ],
   "id": "9f2aa3ff21601cd3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw uniques (top):\n",
      "Label\n",
      "FTP-BruteForce    179244\n",
      "SSH-Bruteforce     20490\n",
      "Benign               266\n",
      "Name: count, dtype: int64\n",
      "\n",
      "normalized uniques (top):\n",
      "Label\n",
      "FTP-BRUTEFORCE    179244\n",
      "SSH-BRUTEFORCE     20490\n",
      "BENIGN               266\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T09:56:21.698566Z",
     "start_time": "2026-02-03T09:55:42.393678Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np, os, gc, pandas as pd\n",
    "\n",
    "csv_files = sorted([f for f in os.listdir(path) if f.endswith(\".csv\")])\n",
    "\n",
    "# create fresh y memmap\n",
    "y_correct = np.memmap(\"y_correct.dat\", dtype=\"uint8\", mode=\"w+\", shape=(n,))\n",
    "\n",
    "current_idx = 0\n",
    "chunk_size = 100_000\n",
    "\n",
    "for i, file in enumerate(csv_files, 1):\n",
    "    fp = os.path.join(path, file)\n",
    "    print(f\"Rebuilding y {i}/{len(csv_files)}: {file}\")\n",
    "\n",
    "    for chunk in pd.read_csv(fp, usecols=[\"Label\"], chunksize=chunk_size):\n",
    "        # remove repeated header rows if any\n",
    "        lab = chunk[\"Label\"].astype(str)\n",
    "        lab = lab[lab != \"Label\"]\n",
    "\n",
    "        if len(lab) == 0:\n",
    "            continue\n",
    "\n",
    "        lab_norm = lab.str.strip().str.upper()\n",
    "\n",
    "        # benign = exactly BENIGN after normalization\n",
    "        y_chunk = (lab_norm != \"BENIGN\").astype(np.uint8).values\n",
    "\n",
    "        end = current_idx + len(y_chunk)\n",
    "        y_correct[current_idx:end] = y_chunk\n",
    "        current_idx = end\n",
    "\n",
    "    print(\"  current_idx:\", f\"{current_idx:,}\")\n",
    "\n",
    "y_correct.flush()\n",
    "print(\"DONE. wrote:\", current_idx, \"expected:\", n)\n"
   ],
   "id": "3869507cf15a4e41",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rebuilding y 1/10: 02-14-2018.csv\n",
      "  current_idx: 1,048,575\n",
      "Rebuilding y 2/10: 02-15-2018.csv\n",
      "  current_idx: 2,097,150\n",
      "Rebuilding y 3/10: 02-16-2018.csv\n",
      "  current_idx: 3,145,724\n",
      "Rebuilding y 4/10: 02-20-2018.csv\n",
      "  current_idx: 11,094,472\n",
      "Rebuilding y 5/10: 02-21-2018.csv\n",
      "  current_idx: 12,143,047\n",
      "Rebuilding y 6/10: 02-22-2018.csv\n",
      "  current_idx: 13,191,622\n",
      "Rebuilding y 7/10: 02-23-2018.csv\n",
      "  current_idx: 14,240,197\n",
      "Rebuilding y 8/10: 02-28-2018.csv\n",
      "  current_idx: 14,853,268\n",
      "Rebuilding y 9/10: 03-01-2018.csv\n",
      "  current_idx: 15,184,368\n",
      "Rebuilding y 10/10: 03-02-2018.csv\n",
      "  current_idx: 16,232,943\n",
      "DONE. wrote: 16232943 expected: 16233002\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T09:56:30.309864Z",
     "start_time": "2026-02-03T09:56:30.264883Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y = np.memmap(\"y_correct.dat\", dtype=\"uint8\", mode=\"r\", shape=(n,))\n",
    "print(\"benign:\", int((y==0).sum()), \"attack:\", int((y==1).sum()))"
   ],
   "id": "865f82f7a56a5a75",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "benign: 13484767 attack: 2748235\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T10:02:25.545586Z",
     "start_time": "2026-02-03T10:02:25.445043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "idx = np.arange(n)\n",
    "train_benign_idx = idx[y == 0] "
   ],
   "id": "cc217c0fe52ca0f6",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T10:03:55.158863Z",
     "start_time": "2026-02-03T10:03:52.755394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "idx = np.arange(n)\n",
    "benign_idx = idx[y == 0]\n",
    "attack_idx = idx[y == 1]\n",
    "\n",
    "# Split benign into train + benign_holdout\n",
    "train_benign_idx, benign_holdout_idx = train_test_split(\n",
    "    benign_idx, test_size=0.30, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "# Val/Test = benign_holdout + all attacks (then split)\n",
    "val_idx, test_idx = train_test_split(\n",
    "    np.concatenate([benign_holdout_idx, attack_idx]),\n",
    "    test_size=0.50, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "print(\"train benign:\", len(train_benign_idx))\n",
    "print(\"val:\", len(val_idx), \"attack%:\", float((y[val_idx]==1).mean()))\n",
    "print(\"test:\", len(test_idx), \"attack%:\", float((y[test_idx]==1).mean()))\n",
    "\n",
    "np.save(\"train_benign_idx.npy\", train_benign_idx)\n",
    "np.save(\"val_idx.npy\", val_idx)\n",
    "np.save(\"test_idx.npy\", test_idx)\n"
   ],
   "id": "9544444de6ae7f02",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train benign: 9439336\n",
      "val: 3396833 attack%: 0.4045559496154212\n",
      "test: 3396833 attack%: 0.404502075904232\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T10:04:28.385140Z",
     "start_time": "2026-02-03T10:04:14.430635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "chunk = 100_000\n",
    "\n",
    "train_sorted = np.sort(train_benign_idx)  # makes sequential reads faster\n",
    "\n",
    "for start in range(0, len(train_sorted), chunk):\n",
    "    sel = train_sorted[start:start+chunk]\n",
    "    scaler.partial_fit(X_raw[sel])\n",
    "\n",
    "import joblib\n",
    "joblib.dump(scaler, \"scaler_train_benign.pkl\")\n",
    "print(\"scaler fitted\")"
   ],
   "id": "2d3fd67dad80c959",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaler fitted\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T10:07:29.310240Z",
     "start_time": "2026-02-03T10:07:04.843506Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, numpy as np\n",
    "\n",
    "out_file = \"X_scaled.dat\"\n",
    "if os.path.exists(out_file):\n",
    "    os.remove(out_file)\n",
    "\n",
    "bytes_needed = int(n) * int(num_features) * np.dtype(np.float32).itemsize\n",
    "with open(out_file, \"wb\") as f:\n",
    "    f.seek(bytes_needed - 1)\n",
    "    f.write(b\"\\0\")\n",
    "\n",
    "X_scaled = np.memmap(out_file, dtype=\"float32\", mode=\"r+\", shape=(n, num_features))\n",
    "\n",
    "chunk_size = 100_000\n",
    "for i in range(0, n, chunk_size):\n",
    "    end = min(i + chunk_size, n)\n",
    "    X_scaled[i:end] = scaler.transform(X_raw[i:end]).astype(np.float32)\n",
    "    if i % 500_000 == 0:\n",
    "        print(f\"scaled {i:,}/{n:,}\")\n",
    "\n",
    "X_scaled.flush()\n",
    "print(\"X_scaled:\", X_scaled.shape)"
   ],
   "id": "280a1d6ce434d402",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaled 0/16,233,002\n",
      "scaled 500,000/16,233,002\n",
      "scaled 1,000,000/16,233,002\n",
      "scaled 1,500,000/16,233,002\n",
      "scaled 2,000,000/16,233,002\n",
      "scaled 2,500,000/16,233,002\n",
      "scaled 3,000,000/16,233,002\n",
      "scaled 3,500,000/16,233,002\n",
      "scaled 4,000,000/16,233,002\n",
      "scaled 4,500,000/16,233,002\n",
      "scaled 5,000,000/16,233,002\n",
      "scaled 5,500,000/16,233,002\n",
      "scaled 6,000,000/16,233,002\n",
      "scaled 6,500,000/16,233,002\n",
      "scaled 7,000,000/16,233,002\n",
      "scaled 7,500,000/16,233,002\n",
      "scaled 8,000,000/16,233,002\n",
      "scaled 8,500,000/16,233,002\n",
      "scaled 9,000,000/16,233,002\n",
      "scaled 9,500,000/16,233,002\n",
      "scaled 10,000,000/16,233,002\n",
      "scaled 10,500,000/16,233,002\n",
      "scaled 11,000,000/16,233,002\n",
      "scaled 11,500,000/16,233,002\n",
      "scaled 12,000,000/16,233,002\n",
      "scaled 12,500,000/16,233,002\n",
      "scaled 13,000,000/16,233,002\n",
      "scaled 13,500,000/16,233,002\n",
      "scaled 14,000,000/16,233,002\n",
      "scaled 14,500,000/16,233,002\n",
      "scaled 15,000,000/16,233,002\n",
      "scaled 15,500,000/16,233,002\n",
      "scaled 16,000,000/16,233,002\n",
      "X_scaled: (16233002, 77)\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T10:07:43.628372Z",
     "start_time": "2026-02-03T10:07:37.387854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MemmapDataset(Dataset):\n",
    "    def __init__(self, X, y, indices):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.indices = np.asarray(indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        idx = self.indices[i]\n",
    "        x = torch.from_numpy(self.X[idx].copy())   # copy avoids memmap view quirks\n",
    "        y = torch.tensor(int(self.y[idx]), dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "train_ds = MemmapDataset(X_scaled, y, train_benign_idx)\n",
    "val_ds   = MemmapDataset(X_scaled, y, val_idx)\n",
    "test_ds  = MemmapDataset(X_scaled, y, test_idx)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=512, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=512, shuffle=False, num_workers=0, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=512, shuffle=False, num_workers=0, pin_memory=True)"
   ],
   "id": "528f4ab410705310",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T10:57:51.679880Z",
     "start_time": "2026-02-03T10:57:51.676690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_dim = X_scaled.shape[1]\n",
    "hidden_dim = min(512, max(128, 2 * input_dim))\n",
    "latent_dim = max(8, min(32, input_dim // 4))"
   ],
   "id": "7e5501542f85c69d",
   "outputs": [],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T10:57:52.041176Z",
     "start_time": "2026-02-03T10:57:52.032166Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total, n = 0.0, 0\n",
    "\n",
    "    for xb, _ in loader:\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        recon, mu, logvar = model(xb)\n",
    "\n",
    "        loss_val, _, _ = loss_fn(xb, recon, mu, logvar)\n",
    "\n",
    "        bs = xb.size(0)\n",
    "        total += loss_val.item() * bs\n",
    "        n += bs\n",
    "\n",
    "    return total / n\n",
    "\n",
    "\n",
    "def fit(model, train_loader, val_loader, loss_fn, optimizer, device, epochs=100, patience=5):\n",
    "    best_val_loss = float(\"inf\")\n",
    "    bad = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total, n = 0.0, 0\n",
    "\n",
    "        for xb, _ in train_loader:\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            recon, mu, logvar = model(xb)\n",
    "\n",
    "            loss_val, recon_l, kld = loss_fn(xb, recon, mu, logvar)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss_val.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            bs = xb.size(0)\n",
    "            total += loss_val.item() * bs\n",
    "            n += bs\n",
    "\n",
    "        train_loss = total / n\n",
    "        val_loss = evaluate(model, val_loader, loss_fn, device)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            bad = 0\n",
    "            torch.save(model.state_dict(), \"best_vae.pth\")\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                model.load_state_dict(torch.load(\"best_vae.pth\", map_location=device))\n",
    "                break"
   ],
   "id": "a4fb47b7bf774b4d",
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T10:57:52.712553Z",
     "start_time": "2026-02-03T10:57:52.709555Z"
    }
   },
   "cell_type": "code",
   "source": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")",
   "id": "b61be5d46865904f",
   "outputs": [],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T10:57:53.815379Z",
     "start_time": "2026-02-03T10:57:53.810377Z"
    }
   },
   "cell_type": "code",
   "source": "device",
   "id": "221cb9957ac05de",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T10:57:54.573928Z",
     "start_time": "2026-02-03T10:57:54.567988Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "864528dca3213bd4",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T11:09:08.278940Z",
     "start_time": "2026-02-03T11:09:07.500183Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "ab43bd3f2a78d122",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xb finite: True\n",
      "recon finite: False\n",
      "mu finite: False\n",
      "logvar finite: False\n",
      "exp(logvar) finite: False min/max: nan nan\n",
      "t finite: False t min/max: nan nan\n"
     ]
    }
   ],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T11:04:33.913638Z",
     "start_time": "2026-02-03T10:57:55.692539Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "58a1105e4e2944ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | train_loss=nan | val_loss=nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[82]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m50\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpatience\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m5\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[78]\u001B[39m\u001B[32m, line 34\u001B[39m, in \u001B[36mfit\u001B[39m\u001B[34m(model, train_loader, val_loader, loss_fn, optimizer, device, epochs, patience)\u001B[39m\n\u001B[32m     31\u001B[39m loss_val, recon_l, kld = loss_fn(xb, recon, mu, logvar)\n\u001B[32m     33\u001B[39m optimizer.zero_grad(set_to_none=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m---> \u001B[39m\u001B[32m34\u001B[39m \u001B[43mloss_val\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     35\u001B[39m torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001B[32m5.0\u001B[39m)\n\u001B[32m     36\u001B[39m optimizer.step()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\TRACE\\.venv\\Lib\\site-packages\\torch\\_tensor.py:581\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    571\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    572\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    573\u001B[39m         Tensor.backward,\n\u001B[32m    574\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    579\u001B[39m         inputs=inputs,\n\u001B[32m    580\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m581\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mautograd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    582\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\n\u001B[32m    583\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\TRACE\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    342\u001B[39m     retain_graph = create_graph\n\u001B[32m    344\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    345\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    346\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m347\u001B[39m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    348\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    349\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    350\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    351\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    352\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    353\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    354\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    355\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\TRACE\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    823\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    824\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m825\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_execution_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[32m    826\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    827\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    828\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    829\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T11:09:38.534217Z",
     "start_time": "2026-02-03T11:09:38.529005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import importlib, inspect, VAE_Model\n",
    "importlib.reload(VAE_Model)\n",
    "\n",
    "print(\"IMPORTING:\", VAE_Model.__file__)\n",
    "print(inspect.getsource(VAE_Model.Model.encode))\n",
    "print(inspect.getsource(VAE_Model.Model.reparameterize))"
   ],
   "id": "d52711685828dda",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTING: C:\\Users\\mjaye\\PycharmProjects\\TRACE\\VAE_Model.py\n",
      "    def encode(self, x):\n",
      "        h = self.encoder(x)\n",
      "        mu = self.mean_layer(h)\n",
      "        logvar = self.logvar_layer(h)\n",
      "        logvar = torch.clamp(logvar, -10.0, 10.0)  # stabilizer\n",
      "        return mu, logvar\n",
      "\n",
      "    def reparameterize(self, mu, logvar):\n",
      "        std = torch.exp(0.5 * logvar)\n",
      "        eps = torch.randn_like(std)\n",
      "        return mu + std * eps\n",
      "\n"
     ]
    }
   ],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T11:10:30.325843Z",
     "start_time": "2026-02-03T11:10:30.308643Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "model = VAE_Model.Model(input_dim=77, hidden_dim=256, latent_dim=16).to(device)\n",
    "\n",
    "# Check model params\n",
    "bad = []\n",
    "for name, p in model.named_parameters():\n",
    "    if not torch.isfinite(p).all():\n",
    "        bad.append(name)\n",
    "\n",
    "print(\"Non-finite params:\", bad[:20], \"count=\", len(bad))"
   ],
   "id": "d1713e1e8ad3b63f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-finite params: [] count= 0\n"
     ]
    }
   ],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T11:10:47.376967Z",
     "start_time": "2026-02-03T11:10:46.593022Z"
    }
   },
   "cell_type": "code",
   "source": [
    "xb, _ = next(iter(train_loader))\n",
    "xb = xb.to(device)\n",
    "\n",
    "x = xb\n",
    "for i, layer in enumerate(model.encoder):\n",
    "    x = layer(x)\n",
    "    if not torch.isfinite(x).all():\n",
    "        print(\"Non-finite after encoder layer\", i, layer)\n",
    "        print(\"min/max:\", x.min().item(), x.max().item())\n",
    "        break\n",
    "\n",
    "mu = model.mean_layer(x)\n",
    "logvar = model.logvar_layer(x)\n",
    "print(\"mu finite:\", torch.isfinite(mu).all().item())\n",
    "print(\"logvar finite:\", torch.isfinite(logvar).all().item())\n",
    "print(\"logvar min/max:\", logvar.min().item(), logvar.max().item())"
   ],
   "id": "96df2cee60cb33a6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu finite: True\n",
      "logvar finite: True\n",
      "logvar min/max: -0.8977354168891907 0.5919608473777771\n"
     ]
    }
   ],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T11:11:25.798135Z",
     "start_time": "2026-02-03T11:11:24.884401Z"
    }
   },
   "cell_type": "code",
   "source": [
    "xb, _ = next(iter(train_loader))\n",
    "xb = xb.to(device)\n",
    "\n",
    "recon, mu, logvar = model(xb)\n",
    "\n",
    "print(\"xb finite:\", torch.isfinite(xb).all().item())\n",
    "print(\"recon finite:\", torch.isfinite(recon).all().item())\n",
    "print(\"mu finite:\", torch.isfinite(mu).all().item())\n",
    "print(\"logvar finite:\", torch.isfinite(logvar).all().item())\n",
    "\n",
    "exp_logvar = torch.exp(logvar)\n",
    "t = 1 + logvar - mu.pow(2) - exp_logvar   # the inside of KL\n",
    "\n",
    "print(\"exp(logvar) finite:\", torch.isfinite(exp_logvar).all().item(),\n",
    "      \"min/max:\", exp_logvar.min().item(), exp_logvar.max().item())\n",
    "print(\"t finite:\", torch.isfinite(t).all().item(),\n",
    "      \"t min/max:\", t.min().item(), t.max().item())"
   ],
   "id": "3a9e9cf2dc4305b6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xb finite: True\n",
      "recon finite: True\n",
      "mu finite: True\n",
      "logvar finite: True\n",
      "exp(logvar) finite: True min/max: 0.20704416930675507 4.005405902862549\n",
      "t finite: True t min/max: -2.0831148624420166 -1.7881393432617188e-07\n"
     ]
    }
   ],
   "execution_count": 94
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T11:28:59.453712Z",
     "start_time": "2026-02-03T11:28:58.949685Z"
    }
   },
   "cell_type": "code",
   "source": [
    "N_TRAIN = 500_000   # try 200k / 500k / 1M / 2M\n",
    "\n",
    "train_benign_idx_small = np.random.choice(train_benign_idx, size=N_TRAIN, replace=False)\n",
    "\n",
    "train_ds = MemmapDataset(X_scaled, y, train_benign_idx_small)\n",
    "train_loader = DataLoader(train_ds, batch_size=1024, shuffle=True, pin_memory=True, num_workers=0)\n",
    "\n",
    "print(\"steps/epoch:\", len(train_loader))"
   ],
   "id": "3f0dee2a2a0a6bc3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps/epoch: 489\n"
     ]
    }
   ],
   "execution_count": 101
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T11:29:01.062937Z",
     "start_time": "2026-02-03T11:29:01.054932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import VAE_Model\n",
    "\n",
    "model = VAE_Model.Model(input_dim=77, hidden_dim=256, latent_dim=16).to(device)\n",
    "loss_fn = VAE_Model.TotalLoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=3e-4)"
   ],
   "id": "40e7a60839de9d97",
   "outputs": [],
   "execution_count": 102
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T11:43:59.959567Z",
     "start_time": "2026-02-03T11:42:57.954257Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# split val indices\n",
    "val_benign_idx = val_idx[y[val_idx] == 0]\n",
    "val_attack_idx = val_idx[y[val_idx] == 1]\n",
    "\n",
    "val_benign_loader = DataLoader(MemmapDataset(X_scaled, y, val_benign_idx),\n",
    "                               batch_size=2048, shuffle=False, pin_memory=True, num_workers=0)\n",
    "\n",
    "val_attack_loader = DataLoader(MemmapDataset(X_scaled, y, val_attack_idx),\n",
    "                               batch_size=2048, shuffle=False, pin_memory=True, num_workers=0)\n",
    "\n",
    "print(\"val benign loss:\", evaluate(model, val_benign_loader, loss_fn, device))\n",
    "print(\"val attack loss:\", evaluate(model, val_attack_loader, loss_fn, device))"
   ],
   "id": "f62529868d96f3e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val benign loss: 12.347293849225787\n",
      "val attack loss: 17806.91465356652\n"
     ]
    }
   ],
   "execution_count": 105
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T12:35:50.529446Z",
     "start_time": "2026-02-03T12:02:56.896993Z"
    }
   },
   "cell_type": "code",
   "source": "fit(model, train_loader, val_benign_loader, loss_fn, opt, device=device, epochs=50, patience=20)",
   "id": "8883d6cc8f4cf4d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | train_loss=48.5335 | val_loss=11.9703\n",
      "Epoch 2/50 | train_loss=48.4268 | val_loss=13.0517\n",
      "Epoch 3/50 | train_loss=47.8888 | val_loss=11.8225\n",
      "Epoch 4/50 | train_loss=47.9732 | val_loss=12.0997\n",
      "Epoch 5/50 | train_loss=48.0023 | val_loss=12.1724\n",
      "Epoch 6/50 | train_loss=47.6889 | val_loss=11.7586\n",
      "Epoch 7/50 | train_loss=47.7345 | val_loss=11.6343\n",
      "Epoch 8/50 | train_loss=50.7595 | val_loss=12.1623\n",
      "Epoch 9/50 | train_loss=48.1266 | val_loss=12.6220\n",
      "Epoch 10/50 | train_loss=48.0012 | val_loss=11.7990\n",
      "Epoch 11/50 | train_loss=47.7002 | val_loss=12.9412\n",
      "Epoch 12/50 | train_loss=47.4624 | val_loss=12.1714\n",
      "Epoch 13/50 | train_loss=47.4898 | val_loss=13.6290\n",
      "Epoch 14/50 | train_loss=48.2797 | val_loss=12.6038\n",
      "Epoch 15/50 | train_loss=47.4027 | val_loss=11.5423\n",
      "Epoch 16/50 | train_loss=47.4362 | val_loss=12.4905\n",
      "Epoch 17/50 | train_loss=47.4856 | val_loss=11.7703\n",
      "Epoch 18/50 | train_loss=47.5653 | val_loss=11.8547\n",
      "Epoch 19/50 | train_loss=47.3064 | val_loss=12.2019\n",
      "Epoch 20/50 | train_loss=47.0310 | val_loss=13.7046\n",
      "Epoch 21/50 | train_loss=47.0853 | val_loss=14.9402\n",
      "Epoch 22/50 | train_loss=47.0666 | val_loss=13.8236\n",
      "Epoch 23/50 | train_loss=47.0804 | val_loss=12.0553\n",
      "Epoch 24/50 | train_loss=46.9928 | val_loss=11.7535\n",
      "Epoch 25/50 | train_loss=47.0374 | val_loss=11.4125\n",
      "Epoch 26/50 | train_loss=47.2175 | val_loss=11.4841\n",
      "Epoch 27/50 | train_loss=47.0857 | val_loss=11.2322\n",
      "Epoch 28/50 | train_loss=45.7060 | val_loss=13.4440\n",
      "Epoch 29/50 | train_loss=46.7186 | val_loss=12.8438\n",
      "Epoch 30/50 | train_loss=50.5621 | val_loss=12.6975\n",
      "Epoch 31/50 | train_loss=46.3643 | val_loss=12.4292\n",
      "Epoch 32/50 | train_loss=46.7416 | val_loss=11.4132\n",
      "Epoch 33/50 | train_loss=46.3210 | val_loss=11.5450\n",
      "Epoch 34/50 | train_loss=46.1536 | val_loss=17.9100\n",
      "Epoch 35/50 | train_loss=46.7888 | val_loss=12.0006\n",
      "Epoch 36/50 | train_loss=46.4186 | val_loss=14.7227\n",
      "Epoch 37/50 | train_loss=53.5223 | val_loss=12.6149\n",
      "Epoch 38/50 | train_loss=47.0284 | val_loss=17.7195\n",
      "Epoch 39/50 | train_loss=47.4425 | val_loss=11.5757\n",
      "Epoch 40/50 | train_loss=46.3642 | val_loss=12.3929\n",
      "Epoch 41/50 | train_loss=46.0115 | val_loss=11.0137\n",
      "Epoch 42/50 | train_loss=45.4508 | val_loss=11.4382\n",
      "Epoch 43/50 | train_loss=45.5580 | val_loss=11.7419\n",
      "Epoch 44/50 | train_loss=44.7356 | val_loss=10.9905\n",
      "Epoch 45/50 | train_loss=44.6597 | val_loss=13.4156\n",
      "Epoch 46/50 | train_loss=44.9825 | val_loss=11.2472\n",
      "Epoch 47/50 | train_loss=45.5598 | val_loss=10.9281\n",
      "Epoch 48/50 | train_loss=44.5602 | val_loss=16.6041\n",
      "Epoch 49/50 | train_loss=44.8802 | val_loss=11.1886\n",
      "Epoch 50/50 | train_loss=44.3687 | val_loss=15.6897\n"
     ]
    }
   ],
   "execution_count": 107
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "19d6b3d2e0f46d65"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
